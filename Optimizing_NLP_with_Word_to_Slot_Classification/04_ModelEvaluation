Model performance was evaluated using two complementary metrics: overall token accuracy and slot‑only accuracy. Overall accuracy measures how often the model predicts the correct label across all tokens, including padding and neutral tokens. Slot‑only accuracy focuses exclusively on meaningful slot labels, excluding padding and “outside” tokens that do not carry semantic information. The most important metric for assessing success was slot‑only accuracy, since correctly labeling cities, dates, and times is the core objective of the task. The Transformer‑based model achieved high accuracy on this metric, significantly outperforming simpler baseline approaches explored earlier in the course. This demonstrates that contextual modeling and attention are critical for resolving ambiguities inherent in natural language. A key tradeoff of this approach is model complexity. The Transformer architecture introduces millions of trainable parameters, increasing training time and computational cost. Additionally, while accuracy is high, errors still occur on rare or ambiguous patterns, especially for underrepresented slot categories. The model’s performance is also constrained by the quality and coverage of the training data.
