Model performance was evaluated using R², both on the training data and on the test data. In‑sample R² provided a measure of how much variance in disease progression was explained by the predictors during model fitting, while out‑of‑sample R² assessed predictive accuracy on unseen observations. The use of a test set was critical for identifying whether improvements in model complexity translated into better real‑world performance. What mattered most in assessing success was not maximizing in‑sample R², but achieving a stable and interpretable model with reasonable out‑of‑sample performance. The reduced model demonstrated that removing statistically insignificant variables did not substantially harm predictive power and, in some cases, improved generalization by reducing noise and overfitting. Several tradeoffs and limitations were evident. Linear regression assumes linear relationships, constant variance of errors, and independence among predictors. Some predictors showed limited statistical significance, suggesting weak linear relationships with the outcome. Additionally, while variable removal improved interpretability, it risked excluding predictors that might contribute in nonlinear or interaction effects not captured by the model.
