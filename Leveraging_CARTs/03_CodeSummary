The code began by importing core numerical, data manipulation, visualization, and machine learning libraries, including NumPy, pandas, matplotlib, scikit‑learn, and XGBoost. These libraries supported data handling, model construction, evaluation, and interpretability. The dataset was loaded from a CSV file into a pandas DataFrame and inspected to confirm data types, dimensionality, and completeness. The feature matrix (X) was created by removing the target variable, while the label vector (y) was derived from the outcome column. Because XGBoost requires numeric class labels, the binary target was encoded into integer form. The data was then split into training and test sets using a 70/30 split, ensuring that model evaluation reflected out‑of‑sample performance. An evaluation set was explicitly defined to support early stopping, allowing the model to halt training once performance stopped improving on unseen data. An XGBClassifier was instantiated with carefully chosen hyperparameters, including learning rate, maximum tree depth, number of estimators, and regularization parameters. The model was trained iteratively, with performance monitored using the area under the ROC curve (AUC). After training, predictions were generated for the test set, and multiple evaluation metrics were computed. Finally, feature importance was extracted and visualized to understand which variables most influenced the model’s decisions.
