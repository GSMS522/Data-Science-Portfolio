Model performance was evaluated by comparing predicted ratings against held‑out observed ratings using mean squared error–based metrics and derived R² values. A baseline predictor, defined as the global average rating from the training set, was used as a reference point to assess whether the collaborative filtering model provided meaningful improvement over a naïve approach. The most important aspect of evaluation was out‑of‑sample performance, as this reflects how well the model generalizes to unseen user-item interactions. In‑sample performance alone can be misleading in matrix factorization problems because increasing model flexibility almost always improves training fit, even when predictive accuracy deteriorates. The comparison between training and validation results highlighted the tradeoff between expressiveness and generalization. Several limitations emerged from the evaluation process. The sparsity of the data makes estimates sensitive to how training and validation splits are constructed, and results can vary depending on which observations are withheld. Additionally, low‑rank models assume linear structure in latent preferences, which may not fully capture complex or nonlinear listening behavior. These tradeoffs are inherent to collaborative filtering systems and must be balanced carefully in practice.
