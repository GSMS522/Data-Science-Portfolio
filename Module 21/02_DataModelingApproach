At a high level, the assignment used a labeled corpus of airline travel queries, where each query is paired with a sequence of slot labels aligned word‑by‑word. The data includes a wide variety of travel‑related concepts such as cities, times, dates, airlines, and fares, resulting in over one hundred distinct slot categories. This diversity increases the difficulty of the task, as the model must learn subtle distinctions between similar semantic roles. The primary modeling technique applied was a Transformer encoder–based neural network. Unlike simpler models that treat text as unordered collections of words, the Transformer architecture processes entire sequences at once and learns contextualized representations of each token. Positional embeddings were used to encode word order, while self‑attention allowed each word to dynamically incorporate information from other words in the sentence. This approach is well suited to slot filling because the label for a given word often depends on surrounding words. For example, the same city name may represent either a departure or arrival location depending on context. The Transformer’s attention mechanism enables the model to capture these dependencies directly, rather than relying on handcrafted features or fixed‑window heuristics.
