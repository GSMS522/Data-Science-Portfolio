Model performance was evaluated using classification accuracy on the held‑out test dataset. Accuracy was an appropriate metric given the balanced class distribution and binary nature of the task. The final model achieved approximately 87% test accuracy, exceeding the minimum performance threshold and indicating strong generalization beyond the training data. The most important factor in assessing success was the gap between training and validation performance. While training accuracy continued to improve across epochs, validation accuracy plateaued and eventually declined slightly, signaling the onset of overfitting. The inclusion of dropout helped control this effect, but the results highlighted the tradeoff between model capacity and generalization. A key limitation of the approach is the loss of word order and contextual meaning. Multi‑hot encoding treats text as an unordered collection of tokens, ignoring syntactic structure and negation effects. Additionally, rare words outside the capped vocabulary were discarded, potentially removing sentiment‑bearing information. Despite these limitations, the model performed well, demonstrating that even simple representations can be effective when combined with sufficient data and non‑linear models.
