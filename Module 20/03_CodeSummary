The code implemented a complete end‑to‑end natural language processing pipeline. It began by loading and preprocessing the dataset, converting categorical sentiment labels into numerical form, and partitioning the data into training and test sets. Exploratory checks confirmed that the sentiment classes were balanced, reducing the risk of biased predictions. Key libraries included pandas and NumPy for data handling, scikit‑learn for dataset splitting, and TensorFlow / Keras for text vectorization and model construction. The TextVectorization layer played a central role by transforming raw text into fixed‑length numeric vectors using multi‑hot encoding over a capped vocabulary size. The neural network architecture consisted of an input layer matching the vectorized text dimension, a dense hidden layer with ReLU activation to learn non‑linear feature interactions, a dropout layer to mitigate overfitting, and a final dense layer with softmax activation to output class probabilities. The model was compiled using the Adam optimizer and categorical cross‑entropy loss, then trained for multiple epochs with validation on the test set.
